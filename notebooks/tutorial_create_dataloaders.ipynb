{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Dataloaders using `torch-em`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tutorial is focused on creating dataloaders using `torch-em` for various segmentation tasks. Let's get started.\n",
    "The first thing to do would be to make sure that we have `torch-em` installed and accessible in the kernel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yayy, we found 'torch-em'. Start creating your dataloaders already\n"
     ]
    }
   ],
   "source": [
    "URL = \"https://github.com/constantinpape/torch-em#installation\"\n",
    "try:\n",
    "    import torch_em\n",
    "    print(\"Yayy, we found 'torch-em'. Start creating your dataloaders already. Skip to Step 1.\")\n",
    "except ModuleNotFoundError:\n",
    "    print(f\"'torch-em' was not found. Please install it from {URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the script above suggests to install `torch-em`, please go ahead and install it first.\n",
    "\n",
    "NOTE: In case you are using Google Colab / Kaggle, the installation is mentioned below, we recommend installing the repositories from [source](https://github.com/constantinpape/torch-em?tab=readme-ov-file#from-source) for best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's try and check again if `torch-em` is installed\n",
    "import torch_em\n",
    "# TODO: need to how this works on kaggle / google colab and what's missing here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, this should not throw any errors. If there are some modules missing, please go ahead and install them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Let's explore our datasets\n",
    "\n",
    "We will create dataloaders from three different open-source datasets, and see how to create dataloaders for training a UNet architecture. The choice of datasets are following:\n",
    "\n",
    "1. DSB (Nuclei Segmentation in Light Microscopy: Caicedo et al. - https://doi.org/10.1038/s41592-019-0612-7)\n",
    "2. Covid IF (Nuclei and Cell Segmentation in Immunofluorescence: Pape et al. - https://doi.org/10.1002/bies.202000257)\n",
    "3. PlantSeg (Cell Segmentation in Confocal and Light-Sheet Microscopy: Wolny et al. - https://doi.org/10.7554/eLife.57613)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fetch_datasets(dataset_name, path):\n",
    "    if dataset_name == \"dsb\":\n",
    "        from torch_em.data.datasets.dsb import _download_dsb\n",
    "        _download_dsb(path, \"reduced\", download=True)\n",
    "        data_path = path\n",
    "\n",
    "    elif dataset_name == \"covid_if:\n",
    "        from torch_em.data.datasets.covid_if import _download_covid_if\n",
    "        _download_covid_if(path, download=True)\n",
    "        data_path = path\n",
    "\n",
    "    elif dataset_name == \"plantseg\":\n",
    "        # let's test for root\n",
    "        from torch_em.data.datasets.plantseg import _require_plantseg_data\n",
    "        data_path = _require_plantseg_data(path, download=True, name=\"root\", split=\"train\")\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Oops, download is not enabled for {dataset_name}.\")\n",
    "\n",
    "    return data_path\n",
    "\n",
    "\n",
    "def plot_samples(image, label):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2)\n",
    "    ax[0].imshow(image)\n",
    "    ax[1].imshow(label)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset is downloaded. Let's explore them quickly before proceeding to create the dataset object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For DSB\n",
    "image_paths = ...\n",
    "label_paths = ...\n",
    "\n",
    "print(\"The image extension seems to be:\", os.path.splitext(image_paths[0]))\n",
    "print(\"The label extension seems to be:\", os.path.splitext(label_paths[0]))\n",
    "\n",
    "# It appears that the images are in tif format. It's a supported data format. Now let's check the data structure.\n",
    "\n",
    "image_shapes = [imageio.imread(path).shape for path in image_paths]\n",
    "label_shapes = [imageio.imread(path).shape for path in label_paths]\n",
    "\n",
    "# It appears that the images has one channel. It's a supported data structure as well. Now let's visualize one image to understand our data better.\n",
    "\n",
    "for image_path, label_path in zip(image_paths, label_paths):\n",
    "    image = imageio.imread(image_path)\n",
    "    label = imageio.imread(label_path)\n",
    "\n",
    "    plot_samples(image, label)\n",
    "\n",
    "    break  # it's enough to check a few samples, feel free to explore the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Covid IF\n",
    "volume_paths = ...\n",
    "\n",
    "print(\"The volume extension seems to be:\", os.path.splitext(image_paths[0]))\n",
    "\n",
    "# It appears that the images are in hdf5 format. It's a supported data format. Now let's check the data structure.\n",
    "\n",
    "# Let's try to open one image first\n",
    "import h5py\n",
    "with h5py.File(image_path[0]) as f:\n",
    "    image = f[\"...\"][:]\n",
    "    label = f[\"...\"][:]\n",
    "\n",
    "    print(image.shape, label.shape)\n",
    "\n",
    "    plot_samples(image, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For PlantSeg\n",
    "volume_paths = ...\n",
    "\n",
    "print(\"The volume extension seems to be:\", os.path.splitext(image_paths[0]))\n",
    "\n",
    "# It appears that the images...\n",
    "\n",
    "# Let's try to open one image first\n",
    "import h5py\n",
    "with h5py.File(image_path[0]) as f:\n",
    "    image = f[\"...\"][:]\n",
    "    label = f[\"...\"][:]\n",
    "\n",
    "    print(image.shape, label.shape)\n",
    "\n",
    "    plot_samples(image, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Let's create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DSB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covid IF dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PlantSeg dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Let's create the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DSB dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covid IF dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PlantSeg dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Let's check our dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_at_chosen_loader(chosen_loader, do_run=False):\n",
    "    if do_run:\n",
    "        save_path = f\"./loader.png\"\n",
    "        print(\"Let's check how the samples look first. We store the images here:\", save_path)\n",
    "        from torch_em.util.debug import check_loader\n",
    "        check_loader(chosen_loader, 8, plt=True, save_path=save_path)\n",
    "    else:\n",
    "        print(f\"There are {len(chosen_loader} samples generated from the loader. Please pass 'do_run=True' to 'look_at_chosen_loader' function.\")\n",
    "        return\n",
    "\n",
    "    for x, y in chosen_loader:\n",
    "        print(x.shape, y.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
